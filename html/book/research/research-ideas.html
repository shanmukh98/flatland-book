
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Research Ideas</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/custom.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Flatland Environment" href="../faq/env.html" />
    <link rel="prev" title="Frame skipping and action masking" href="baselines/action_masking_and_skipping.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/flatland-logo.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to Flatland
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../getting-started/installation.html">
   Installing Flatland
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting-started/env.html">
   Flatland Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting-started/The_Rail_Environment.html">
   Using
   <code class="docutils literal notranslate">
    <span class="pre">
     RailEnv
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting-started/sequential-agent.html">
   Sequential Agent
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Environment
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../env/observations.html">
   Provided Observations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../env/level_generation.html">
   Level Generation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../env/environment_information.html">
   Environment information
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../env/custom_observations.html">
   Custom observations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../env/speed_profiles.html">
   Speed profiles
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../env/updates.html">
   Updates
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../env/updates/Agent-Close-Following.html">
     Unordered Close Following
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://flatlandrl-docs.aicrowd.com/flatland.html">
   API Reference
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tuts/or.html">
   Operations Research
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../tuts/rl.html">
   Reinforcement Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../tuts/rl/single-agent.html">
     Single agent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../tuts/rl/multi-agent.html">
     Multiple Agents
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Challenges
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../challenges/amld2021.html">
   AMLD 2021
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../challenges/amld2021/envconfig.html">
     Environment Configurations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../challenges/amld2021/eval.html">
     Evaluation Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../challenges/amld2021/first-submission.html">
     Making a Submission
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../challenges/neurips2020-challenge.html">
   Neurips 2020
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../challenges/neurips2020/envconfig.html">
     Environment Configurations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../challenges/neurips2020/eval.html">
     Evaluation Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../challenges/neurips2020/first-submission.html">
     Making a Submission
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../challenges/neurips2020/faq.html">
     FAQs
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../challenges/halloffame.html">
   Hall of Fame
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="baselines.html">
   RLlib Baselines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="baselines/getting-started.html">
     Getting started
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="baselines/centralized_critic.html">
     Centralized Critic PPO
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="baselines/imitation_learning.html">
     Imitation Learning Training
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="baselines/global_density_obs.html">
     Global Density Observation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="baselines/combined_tree_local_conflict_obs.html">
     Combined Observation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="baselines/action_masking_and_skipping.html">
     Frame skipping and action masking
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Research Ideas
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  FAQ
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../faq/env.html">
   Flatland Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../faq/research.html">
   Flatland Research
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Miscellaneous
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../misc/external-resources.html">
   Community Projects
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../misc/contributing.html">
   Contributing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/contributing-code.html">
     Writing code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../misc/contributing-doc.html">
     Writing documentation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../misc/credits.html">
   Credits
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/book/research/research-ideas.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#publications">
   Publications
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#strong-generalization-and-efficiency-in-neural-programs-li-et-al">
     Strong Generalization and Efficiency in Neural Programs (Li et al.)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparison-of-path-planning-methods-for-a-multi-robot-team-hvezda">
     Comparison of path planning methods for a multi-robot team (Hvƒõzda)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chip-placement-with-deep-reinforcement-learning-mirhoseini-et-al">
     Chip Placement with Deep Reinforcement Learning (Mirhoseini et al.)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shunting-trains-with-deep-reinforcement-learning-peer-e-menkovski-v-zhang-y-lee-w-j-2018">
     Shunting Trains with Deep Reinforcement Learning  (Peer, E., Menkovski, V., Zhang, Y., &amp; Lee, W-J. (2018))
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="research-ideas">
<h1>Research Ideas<a class="headerlink" href="#research-ideas" title="Permalink to this headline">¬∂</a></h1>
<p>This page lists publications, blogs posts and other resources that could be interesting in the context of the Flatland environment. These ideas are currently unexplored, and anyone is welcome to research them. Conversely, feel free to <a class="reference external" href="https://gitlab.aicrowd.com/flatland/flatland-book/blob/master/research/research-ideas.md">submit PRs</a> if you have interesting ideas that you are willing to share with the community!</p>
<div class="section" id="publications">
<h2>Publications<a class="headerlink" href="#publications" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="strong-generalization-and-efficiency-in-neural-programs-li-et-al">
<h3>Strong Generalization and Efficiency in Neural Programs (Li et al.)<a class="headerlink" href="#strong-generalization-and-efficiency-in-neural-programs-li-et-al" title="Permalink to this headline">¬∂</a></h3>
<p>In this paper, DeepMind trains a neural program (a program ‚Äúlearned‚Äù using machine learning) to solve various tasks: sorting lists, solving knapsack problems‚Ä¶ In some settings, the resulting algorithms perform better than the original hand-coded ones.</p>
<p>Could we use this for Flatland: instead of writing a planning solution ourselves, and instead of training an RL agent that learns to take decisions, what about we train a full program that learns to plan?</p>
<blockquote>
<div><p>We study the problem of learning efficient algorithms that strongly generalize in the framework of neural program induction. By carefully designing the input / output interfaces of the neural model and through imitation, we are able to learn models that produce correct results for arbitrary input sizes, achieving strong generalization. Moreover, by using reinforcement learning, we optimize for program efficiency metrics, and discover new algorithms that surpass the teacher used in imitation. With this, our approach can learn to outperform custom-written solutions for a variety of problems, as we tested it on sorting, searching in ordered lists and the NP-complete 0/1 knapsack problem, which sets a notable milestone in the field of Neural Program Induction. As highlights, our learned model can perform sorting perfectly on any input data size we tested on, with O(nlogn) complexity, whilst outperforming hand-coded algorithms, including quick sort, in number of operations even for list sizes far beyond those seen during training.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://arxiv.org/abs/2007.03629">üîó arXiv</a></strong></p></li>
<li><p><strong><a class="reference external" href="https://news.ycombinator.com/item?id=23788298">üîó HN discussion</a></strong></p></li>
<li><p><strong>üåü Found by: Florian</strong></p></li>
</ul>
</div>
<div class="section" id="comparison-of-path-planning-methods-for-a-multi-robot-team-hvezda">
<h3>Comparison of path planning methods for a multi-robot team (Hvƒõzda)<a class="headerlink" href="#comparison-of-path-planning-methods-for-a-multi-robot-team-hvezda" title="Permalink to this headline">¬∂</a></h3>
<p>Comparison of path planning methods for a multi-robot team. Very similar to flatland environment, apart from the junction constraints, same speed, no failures‚Ä¶? (AFAICT). The algorithm assumes that the resource graph is constructed such that resources are of two types: intersection resources with capacity 1 and lane resources with capacity 1 or greater. Another assumption is also that if multiple agents are present on the same resource then they are all traveling in the same direction and their order does not change, meaning they cannot overtake each other. The idea is that the lanes are not wide enough for two agents to drive in parallel but long enough so that agents can drive behind each other.</p>
<blockquote>
<div><p>This master thesis discusses the topic of multi-agent pathplanning. For this reason several algorithms were picked and described in the first part of this thesis. All algorithms were implemented in C++ and from experience from working with these algorithms several modifications and improvements were proposed and implemented. The second part of the thesis elaborates on the results of experiments performed on the basic versions of the algorithms as well as the improvements and discusses their effect. This part discusses the potential applications the algorithms as well. All algorithms were tested on the map of robotic warehouse as well as grid maps from pc games.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://core.ac.uk/download/pdf/84833259.pdf">üîó Paper</a></strong></p></li>
<li><p><strong>üåü Found by: Jeremy</strong></p></li>
</ul>
</div>
<div class="section" id="chip-placement-with-deep-reinforcement-learning-mirhoseini-et-al">
<h3>Chip Placement with Deep Reinforcement Learning (Mirhoseini et al.)<a class="headerlink" href="#chip-placement-with-deep-reinforcement-learning-mirhoseini-et-al" title="Permalink to this headline">¬∂</a></h3>
<p>This work from Google AI uses reinforcement learning to place chips on a PCB. This placement problem could be seen as a planning problem, if you consider the electric lines as trains which need to reach a destination without blocking each other.</p>
<blockquote>
<div><p>In this work, we present a learning-based approach to chip placement, one of the most complex and time-consuming stages of the chip design process. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. To achieve these results, we pose placement as a Reinforcement Learning (RL) problem and train an agent to place the nodes of a chip netlist onto a chip canvas. To enable our RL policy to generalize to unseen blocks, we ground representation learning in the supervised task of predicting placement quality. By designing a neural architecture that can accurately predict reward across a wide variety of netlists and their placements, we are able to generate rich feature embeddings of the input netlists. We then use this architecture as the encoder of our policy and value networks to enable transfer learning. (‚Ä¶)</p>
</div></blockquote>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://arxiv.org/pdf/2004.10746.pdf">üîó arXiv</a></strong></p></li>
<li><p><strong><a class="reference external" href="https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html">üîó Google AI blog post</a></strong></p></li>
<li><p><strong>üåü Found by: Florian</strong></p></li>
</ul>
</div>
<div class="section" id="shunting-trains-with-deep-reinforcement-learning-peer-e-menkovski-v-zhang-y-lee-w-j-2018">
<h3>Shunting Trains with Deep Reinforcement Learning  (Peer, E., Menkovski, V., Zhang, Y., &amp; Lee, W-J. (2018))<a class="headerlink" href="#shunting-trains-with-deep-reinforcement-learning-peer-e-menkovski-v-zhang-y-lee-w-j-2018" title="Permalink to this headline">¬∂</a></h3>
<blockquote>
<div><p>The Train Unit Shunting Problem (TUSP) is a
difficult sequential decision making problem faced by Dutch
Railways (NS). Current heuristic solutions under study at NS fall
short in accounting for uncertainty during plan execution and
do not efficiently support replanning. Furthermore, the resulting
plans lack consistency. We approach the TUSP by formulating
it as a Markov Decision Process and develop an image-like
state space representation that allows us to develop a Deep
Reinforcement Learning (DRL) solution. The Deep Q-Network
efficiently reduces the state space and develops an on-line strategy
for the TUSP capable of dealing with uncertainty and delivering
significantly more consistent solutions compared to approaches
currently being developed by NS.</p>
</div></blockquote>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://pure.tue.nl/ws/portalfiles/portal/111663028/Peer2018shunting.pdf">PDF Link - TUE</a></strong></p></li>
<li><p><strong>Peer, E., Menkovski, V., Zhang, Y., &amp; Lee, W-J. (2018). Shunting trains with deep reinforcement learning. In 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC) (pp. 3063-3068). [8616516] IEEE-SMC. https://doi.org/10.1109/SMC.2018.00520</strong></p></li>
<li><p><strong>Found by: Adrian</strong></p></li>
</ul>
<!--

https://arxiv.org/abs/1802.08757

https://www.reddit.com/r/MachineLearning/comments/lbogsa/r_learning_improvement_heuristics_for_vehicle/

https://github.com/Wadaboa/flatland-challenge/raw/master/report/report.pdf

https://research.tue.nl/en/publications/combining-deep-reinforcement-learning-with-search-heuristics-for-

https://arxiv.org/abs/2010.04740

Jonas Walter: https://www.witpress.com/elibrary/tdi/4/4/2733

https://dspace.cvut.cz/bitstream/handle/10467/87776/F3-BP-2020-Ryzner-Filip-BP_FILIP_RYZNER_2020.pdf

https://maro.readthedocs.io/en/v0.1/index.html

https://github.com/giulic3/flatland-challenge-marl
https://amslaurea.unibo.it/20412/1/thesis_giulia_cantini.pdf

### Neural Combinatorial Optimization with Reinforcement Learning

https://arxiv.org/abs/1611.09940 (from Jeremy)

> This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items.

Couple of implementations (2-3 years old):
- https://github.com/MichelDeudon/neural-combinatorial-optimization-rl-tensorflow  (I just ran training; but I think it needs an old version of Google OR Tools for testing, or a bit of updating itself)
- https://github.com/pemami4911/neural-combinatorial-rl-pytorch  Not tried this yet. (Florian: this guy generally does good work)
- https://github.com/MichelDeudon/neural-combinatorial-optimization-rl-tensorflow

### Attention, Learn to Solve Routing Problems!

https://arxiv.org/pdf/1803.08475v3.pdf (from Nilabha)

> The recently presented idea to learn heuristics for combinatorial optimization
  problems is promising as it can save costly development. However, to push this
  idea towards practical implementation, we need better models and better ways
  of training. We contribute in both directions: we propose a model based on attention layers with benefits over the Pointer Network and we show how to train
  this model using REINFORCE with a simple baseline based on a deterministic
  greedy rollout, which we find is more efficient than using a value function. We
  significantly improve over recent learned heuristics for the Travelling Salesman
  Problem (TSP), getting close to optimal results for problems up to 100 nodes.
  With the same hyperparameters, we learn strong heuristics for two variants of the
  Vehicle Routing Problem (VRP), the Orienteering Problem (OP) and (a stochastic variant of) the Prize Collecting TSP (PCTSP), outperforming a wide range of
  baselines and getting results close to highly optimized and specialized algorithms.


### Conflict-free route planning in dynamic environments

https://www.researchgate.net/publication/221063712_Conflict-free_route_planning_in_dynamic_environments (form Jeremy)

> Motion  planning  for  multiple  robots  is  tractable in  case  we  can  assume  a  roadmap  on  which  all  the  robotstravel, which is often the case in many automated guided vehicledomains,  such  as  factory  floors  or  container  terminals.  Wepresent  anO(nvlog(nv) +n2v)(nthe  number  of  nodes,vthe  number  of  vehicles)  route  planning  algorithm  for  a  singlerobot,  which  can  find  the  minimum-time  route  given  a  set  ofexisting  route  plans  that  it  may  not  interfere  with.In  addition,  we  present  an  algorithm  that  can  propagatedelay  through  the  plans  of  the  robots  in  case  one  or  morerobots are delayed. This delay-propagation algorithm allows usto implement a Pareto-optimal plan repair scheme, in which onerobot can improve its route plan without adversely affecting theother robots. We compare this approach to several plan repairschemes  from  the  literature,  which  are  based  on  the  idea  ofgiving  a  higher  priority  to  non-delayed  agents

### Q-Learning in enormous action spaces via amortized approximate maximization

https://arxiv.org/abs/2001.08116 (from Florian)

Action space could be forward/left/right/pause x nb agents?

### An efficient train scheduling algorithm on a single-track railway system

https://link.springer.com/article/10.1007/s10951-018-0558-0

### Traffic prediction with advanced Graph Neural Networks

https://deepmind.com/blog/article/traffic-prediction-with-advanced-graph-neural-networks

--></div>
</div>
</div>


              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="baselines/action_masking_and_skipping.html" title="previous page">Frame skipping and action masking</a>
    <a class='right-next' id="next-link" href="../faq/env.html" title="next page">Flatland Environment</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Flatland Community<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-132885496-4', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>